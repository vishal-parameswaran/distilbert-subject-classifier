{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from tqdm.notebook import tqdm\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = {'biology': 0, 'chemistry': 1, 'computer': 2, 'maths': 3, 'physics': 4, 'social sciences': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(r'../../data/final/final_test.csv')\n",
    "test_df.isna().sum()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on Models\n",
    "\n",
    "We will test only on the test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")\n",
    "classifier = pipeline('zero-shot-classification', model='typeform/distilbert-base-uncased-mnli',tokenizer=tokenizer,device='cuda:0')\n",
    "candidate_labels = list(map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abeee89c260496590ec0a63e231df9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "results = []\n",
    "CHUNK_SIZE = 256\n",
    "for chunk in tqdm(range(test_df.shape[0] // CHUNK_SIZE + 1)):\n",
    "    descr = test_df[CHUNK_SIZE * chunk: (chunk+1) * CHUNK_SIZE]['text'].to_list()\n",
    "    res = classifier(descr,candidate_labels, truncation=True)\n",
    "    results += res\n",
    "    # time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Getting Started This chapter will be about get...</td>\n",
       "      <td>[computer, maths, social sciences, chemistry, ...</td>\n",
       "      <td>[0.38398057222366333, 0.17693284153938293, 0.1...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.383981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you are a graphic or web designer and want ...</td>\n",
       "      <td>[computer, social sciences, biology, chemistry...</td>\n",
       "      <td>[0.23922796547412872, 0.16352412104606628, 0.1...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.239228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This approach is very common because it is so ...</td>\n",
       "      <td>[computer, social sciences, maths, biology, ch...</td>\n",
       "      <td>[0.26167187094688416, 0.17918966710567474, 0.1...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.261672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To  deal  with  this  problem,  Centralized  V...</td>\n",
       "      <td>[chemistry, physics, maths, computer, biology,...</td>\n",
       "      <td>[0.17761172354221344, 0.17454923689365387, 0.1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>However, this setup also has some serious down...</td>\n",
       "      <td>[computer, maths, social sciences, chemistry, ...</td>\n",
       "      <td>[0.8039895296096802, 0.0479842834174633, 0.046...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.803990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Getting Started This chapter will be about get...   \n",
       "1  If you are a graphic or web designer and want ...   \n",
       "2  This approach is very common because it is so ...   \n",
       "3  To  deal  with  this  problem,  Centralized  V...   \n",
       "4  However, this setup also has some serious down...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [computer, maths, social sciences, chemistry, ...   \n",
       "1  [computer, social sciences, biology, chemistry...   \n",
       "2  [computer, social sciences, maths, biology, ch...   \n",
       "3  [chemistry, physics, maths, computer, biology,...   \n",
       "4  [computer, maths, social sciences, chemistry, ...   \n",
       "\n",
       "                                              scores  label     score  \n",
       "0  [0.38398057222366333, 0.17693284153938293, 0.1...      2  0.383981  \n",
       "1  [0.23922796547412872, 0.16352412104606628, 0.1...      2  0.239228  \n",
       "2  [0.26167187094688416, 0.17918966710567474, 0.1...      2  0.261672  \n",
       "3  [0.17761172354221344, 0.17454923689365387, 0.1...      1  0.177612  \n",
       "4  [0.8039895296096802, 0.0479842834174633, 0.046...      2  0.803990  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_pd = pd.DataFrame(results)\n",
    "results_pd.rename(columns={'sequence':'text'},inplace=True)\n",
    "results_pd['label'] = results_pd['labels'].apply(lambda x: map[x[0]])\n",
    "results_pd['score'] = results_pd['scores'].apply(lambda x: x[0])\n",
    "results_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        biology       0.45      0.16      0.24     15988\n",
      "      chemistry       0.52      0.47      0.49     20678\n",
      "       computer       0.25      0.36      0.29      8754\n",
      "          maths       0.22      0.25      0.23     26661\n",
      "        physics       0.25      0.25      0.25     10306\n",
      "social sciences       0.26      0.30      0.28     25695\n",
      "\n",
      "       accuracy                           0.30    108082\n",
      "      macro avg       0.32      0.30      0.30    108082\n",
      "   weighted avg       0.32      0.30      0.30    108082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df['label'],results_pd['label'], target_names=[str(l) for l in map.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using a base model for re-training with the base model trained on depression dataset\n",
    "labels = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        \n",
    "        # Setting the parameters for the BERT tokenizer with max length = 512 and converting them to pytorch tensors\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in tqdm(df['text'])]\n",
    "        \n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module, PyTorchModelHubMixin):\n",
    "\n",
    "    def __init__(self, config: dict, dropout=0.1 ):\n",
    "        \n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 192)\n",
    "        self.linear1 = nn.Linear(192, 64)\n",
    "        self.linear2 = nn.Linear(64, 6)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        output_1 = self.bert(input_ids= input_id,attention_mask=mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooled_output = hidden_state[:, 0]\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Adding a new hidden layer to the output of BERT model\n",
    "        linear_output1 = self.linear(dropout_output)\n",
    "        linear_output1 = self.gelu(linear_output1)\n",
    "        linear_output1 = self.dropout(linear_output1)\n",
    "        \n",
    "        # Adding a new hidden layer to the output of BERT model\n",
    "        linear_output2 = self.linear1(linear_output1)\n",
    "        linear_output2 = self.gelu(linear_output2)\n",
    "        linear_output2 = self.dropout(linear_output2)\n",
    "        \n",
    "        # Adding the sigmoid layer for the model output\n",
    "        final_layer = self.linear2(linear_output2)\n",
    "        \n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34bd0d2344246479c0cda8800d3227a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108082 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26c9479d73c48b18619efe9694922f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = Dataset(test_df)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size = 256)\n",
    "reverse_map = {value: key for key, value in map.items()}\n",
    "config = {\n",
    "    \"id2label\": reverse_map,\n",
    "    \"label2id\": map,\n",
    "    \"problem_type\": \"multi_label_classification\",\n",
    "    \"torch_dtype\": \"float32\",\n",
    "    \"transformers_version\": \"4.38.1\",\n",
    "    \"use_cache\": True\n",
    "}\n",
    "\n",
    "model = BertClassifier(config=config)\n",
    "model.load_state_dict(torch.load(r'../../models/distilbert_model'))\n",
    "model = model.cuda()\n",
    "res = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "        count = 0\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "            \n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            res += output.argmax(dim=1).cpu().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "        biology       0.98      0.99      0.99     15988\n",
      "      chemistry       1.00      0.99      0.99     20678\n",
      "       computer       1.00      0.99      0.99      8754\n",
      "          maths       1.00      1.00      1.00     26661\n",
      "        physics       0.99      0.98      0.99     10306\n",
      "social sciences       0.99      1.00      0.99     25695\n",
      "\n",
      "       accuracy                           0.99    108082\n",
      "      macro avg       0.99      0.99      0.99    108082\n",
      "   weighted avg       0.99      0.99      0.99    108082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df['label'],res, target_names=[str(l) for l in map.keys()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
